from sparse_emb_util import ICUWordPreTokenizer as ICUWordPreTokenizerRs
from pyimpl_icu_word_tokenizer import ICUWordPreTokenizer as ICUWordPreTokenizerPy

# [rs] ICU Word tokenize text use: 0.5409548282623291
# [py] ICU Word tokenize text use: 1.611401081085205
def test_word_tokenze_speed():
    import time

    text = "è¿™æ˜¯ä¸€å¥ä¸­æ–‡ï¼ã€‚This is an English sentence. ì—¬ê¸°ì— í•œêµ­ì–´ ë¬¸ì¥ì´ ìˆìŠµë‹ˆë‹¤.!" * 100
    text2 = "This is a test sentence. è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•å¥å­ã€‚" * 100
    text3 = "Hello World!!. è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•å¥å­ã€‚" * 100
    text4 = "ä½ å¥½ğŸ‘‹ã€‚ä½ å¥½ğŸ‘‹ä½ å¥½ğŸ‘‹ä½ å¥½ğŸ‘‹ä½ å¥½ğŸ‘‹ä½ å¥½ğŸ‘‹" * 100
    texts = [text, text2, text3, text4] * 1_000

    icu_tokenizer_rs = ICUWordPreTokenizerRs()
    start_time = time.time()
    token_lists_rs = icu_tokenizer_rs(texts)
    print(f"[rs] ICU Word tokenize text use: {time.time()-start_time}")

    icu_tokenizer_py = ICUWordPreTokenizerPy()
    start_time = time.time()
    for idx, _text in enumerate(texts):
        token_list = icu_tokenizer_py(_text)

        # ICU4X cutting is right: ['ä½ å¥½', 'ğŸ‘‹', 'ã€‚', 'ä½ å¥½', 'ğŸ‘‹', 'ä½ å¥½', 'ğŸ‘‹', 'ä½ å¥½', 'ğŸ‘‹', ...]
        # ICU4C cutting is wrong: ['ä½ å¥½', 'ğŸ‘‹ã€‚', 'ä½ ', 'å¥½ğŸ‘‹', 'ä½ å¥½', 'ğŸ‘‹ä½ ', 'å¥½ğŸ‘‹', 'ä½ å¥½', 'ğŸ‘‹ä½ ', 'å¥½ğŸ‘‹', 'ä½ å¥½', ...]
        # assert token_lists_rs[idx] == token_list
    print(f"[py] ICU Word tokenize text use: {time.time()-start_time}")

    print()

if __name__ == "__main__":
    test_word_tokenze_speed()
